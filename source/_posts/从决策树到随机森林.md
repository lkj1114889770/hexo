---
title: 从决策树到随机森林
date: 2017-08-24 17:56:19
tags:
	- 机器学习
---
学习了决策树算法之后又接触到了随机森林，随机森林可以说是决策树算法的集成加强版。从分类上来说，随机森林属于机器学习中的集成学习，所谓集成学习，顾名思义就是集成一些方法一起来学习，随机森林就是集成很多决策树来实现其分类或者回归的功能。

<!-- more -->

## 随机森林之“森林”
“森林”表示有很多决策树，每棵决策树都是一个分类器，对于一个输入测试样本，经过森林中的每棵树都会有相应的预测值或者标签，而最终的结果就取决于这些树之间的投票结果作为最终预测结果。每棵随机树都是一个弱分类器，但是通过投票选择，最终组成一个强分类器。
![](http://i.imgur.com/Q9qFsle.png)

## 随机森林之“随机”
随机森林的随机体现在两个地方：一个是构建单棵决策树时的样本选择随机，一个是决策树分裂的时候选择的特征集随机。

1. 假设有N个样本，那么构建某一棵随机数时，放回抽样选择N个样本，称为bootstrap，这样每棵树的训练集都是不同的，当然也会包括重复样本。
2. 在决策树分类的时候，假设每个样本含有M个特征，对于每一棵树，随机抽取m<<M(有不同的取法，常见的有log2（M)，sqrt（M）等）个特征，在每一次进行树枝分裂的时候都从这m个特征中选取最优分裂点。

## 随机森林经典python实现及API总结
python的sklearn模块集成了众多的机器学习算法，其中也包括随机森林（RandomTree），再结合pandas模块，就可以实现随机森林算法的分类或者回归。

    from sklearn.datasets import load_iris
	from sklearn.ensemble import RandomForestClassifier
	import pandas as pd
	import numpy as np
	
	iris = load_iris() #导入鸢尾植物数据集
	df = pd.DataFrame(iris.data, columns=iris.feature_names)
	df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75  #随机选取训练节，大概取3/4
	df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names) #添加从分类变量解码的分类值
	
	train, test = df[df['is_train']==True], df[df['is_train']==False]
	
	features = df.columns[:4]
	clf = RandomForestClassifier(n_jobs=2)
	y, _ = pd.factorize(train['species']) #将分类结果编码成数值
	clf.fit(train[features], y)
	
	preds = iris.target_names[clf.predict(test[features])]
	 #将实际结果与预测结果合并成交叉列表输出
	print(pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])) 

最终的预测结果如下图：

| preds | setosa | versicolor | virginica |
|:-----|:-----|:-----|:-----|:-----|:-----|
| actual |  |  |  |
| setosa | 16 | 0 | 0 |
| versicolor | 0 | 15 | 1 |
| virginca | 0 | 3 | 16 |

当然，每次运行最终预测结果是不一样的，因为测试集和训练集都是每次随机选取的。

	iris = load_iris() #导入鸢尾植物数据集

sklearn模块含有一些机器学习经典的数据集，这里导入了鸢尾植物数据集，导入后iris为字典数据类型，存储了其萼片和花瓣的长宽，一共4个属性，鸢尾植物又分三类。与之相对，iris里有两个属性iris.data，iris.target，data里是一个矩阵，每一列代表了萼片或花瓣的长宽，一共4列，每一列代表某个被测量的鸢尾植物，一共采样了150条记录。

然后又碰到了pandas的一个模块，pandas.Categorical，将一些label转变成categorical variable（分类变量）

     Categorical.from_codes(codes, categories, ordered=False)
这个函数产生一个categorical type根据codes和categories。

pandas.factorizes 则与之相反，Encode input values as an enumerated type or categorical variable。

返回的结果中：
labels : the indexer to the original array
uniques : ndarray (1-d) or Index

python机器学习模块sklearn中的RandomForestClassifier

**sklearn.ensemble**.**RandomForestClassifier**(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)

**n_estimators** : integer, optional (default=10)
&emsp;&emsp;森林中树木数目

**criterion** : string, optional (default=”gini”)
&emsp;&emsp;树枝分裂算法，gini和entropy

**max_features** : int, float, string or None, optional (default=”auto”)
&emsp;&emsp;寻找最优分裂点计算采用的特征数目
If int, then consider max_features features at each split.
If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.
If “auto”, then max_features=sqrt(n_features).
If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).
If “log2”, then max_features=log2(n_features).
If None, then max_features=n_features.
Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.

**max_depth** : integer or None, optional (default=None)
&emsp;&emsp;树的最大深度
The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.

**min_samples_split** : int, float, optional (default=2)
The minimum number of samples required to split an internal node:
If int, then consider min_samples_split as the minimum number.
If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
Changed in version 0.18: Added float values for percentages.

**min_samples_leaf** : int, float, optional (default=1)
The minimum number of samples required to be at a leaf node:
If int, then consider min_samples_leaf as the minimum number.
If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
Changed in version 0.18: Added float values for percentages.

**min_weight_fraction_leaf** : float, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.

**max_leaf_nodes** : int or None, optional (default=None)
Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.

**min_impurity_split** : float,
Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.
Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19 and will be removed in 0.21. Use min_impurity_decrease instead. 

**min_impurity_decrease** : float, optional (default=0.)
A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
The weighted impurity decrease equation is the following:
> N_t / N * (impurity - N_t_R / N_t * right_impurity
>                     - N_t_L / N_t * left_impurity)

where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child.
N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed.
New in version 0.19.

**bootstrap** : boolean, optional (default=True)
Whether bootstrap samples are used when building trees.
oob_score : bool (default=False)
Whether to use out-of-bag samples to estimate the generalization accuracy.

**n_jobs** : integer, optional (default=1)
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.

**random_state** : int, RandomState instance or None, optional (default=None)
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.

**verbose** : int, optional (default=0)
Controls the verbosity of the tree building process.
warm_start : bool, optional (default=False)
When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.

**class_weight** : dict, list of dicts, “balanced”,
“balanced_subsample” or None, optional (default=None) Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.
Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].
The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))
The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.
For multi-output, the weights of each column of y will be multiplied.
Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.

其余函数：

| function | description |
|:-----|:-----|
| apply(X) | Apply trees in the forest to X, return leaf indices. |
| decision_path(X) | Return the decision path in the forest |
| fit(X, y[, sample_weight]) | Build a forest of trees from the training set (X, y). |
| get_params([deep]) | Get parameters for this estimator. |
| predict(X) | Predict class for X. |
| predict_log_proba(X) | Predict class log-probabilities for X. |
| predict_proba(X) | Predict class probabilities for X. |
| score(X, y[, sample_weight]) | Returns the mean accuracy on the given test data and labels. |
| set_params(**params) | Set the parameters of this estimator. |

