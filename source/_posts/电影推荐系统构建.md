---
title: 电影推荐系统构建
date: 2018-01-11 14:12:27
tags:
	- 电影
	- 机器学习
	- 数据挖掘
---

很久没有更新博客了，最近刚做完数据挖掘的大作业，选了一个电影数据集挖掘的课题，做了一个推荐系统，在这里简单地记录一下。
电影推荐系统数据集来源于[kaggle](https://www.kaggle.com/rounakbanik/the-movies-dataset)上的MovieLens完整的45,000条数据，电影数据包括2017年7月前发布的电影，包括270,000个用户的超过26,000,000条评论，以及从GroupLens官方网站获得的评分。基于此电影数据集，完成下面的数据挖掘目标。
<!-- more -->
•	电影数据集处理及可视化分析
•	基于用户投票的推荐算法
•	基于内容的推荐算法
•	基于协同过滤的推荐算法
•	数据库技术的应用
•	简单的电影推荐网站构建
当然这次代码也有很大程度上参考了这个数据集下的大佬分享的kernel，提供了许多不错的精致代码。

## 数据集介绍及分析

movies_metadata.csv: 电影基本信息描述文件，包括 45000部电影的演员、工作人员、情节关键字、预算、收入、海报、发布日期、语言、制作公司、国家、TMDB投票计数和平均投票信息.
keywords.csv: 包含电影的关键词信息，每条数据为json格式.。
credits.csv: 演员和电影工作人员的信息，每条数据为json格式。
links.csv: 包含所有电影TMDB IDs和IMDB IDs 对应信息。
links_small.csv: 9,000部电影的TMDB IDs和IMDB IDs 对应信息.
rating.csv:用户对于所有电影的打分，1-5。
ratings_small.csv: 电影打分子集，700个用户对于9,000部电影的100,000个评分。
针对电影的情况，首先我们看一下电影的平均投票分布，如下图所示，由图中可以看出，电影集中分布在6分左右，也是比较符合实际情况，一般的电影居多，高分电影以及烂片数量相对较少。

	%matplotlib inline
	import pandas as pd
	import numpy as np
	import warnings
	warnings.filterwarnings('ignore')
	import matplotlib.pyplot as plt
	import seaborn as sns
	df = pd.read_csv('movies_metadata.csv')
	df['vote_average'] = df['vote_average'].replace(0, np.nan)
	sns.distplot(df['vote_average'].fillna(df['vote_average'].median()))

![](https://i.imgur.com/jBazUei.png)

	df['year'] = pd.to_datetime(df['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)
	year_gen = pd.DataFrame(df['year'].value_counts()).reset_index()
	year_gen.columns = ['year', 'counts']
	year_gen.drop([87,135,],inplace=True)
	year_gen['year']=year_gen['year'].astype('int')
	plt.plot(year_gen.year,year_gen.counts)

![](https://i.imgur.com/SdNwqOI.png)

从上图中的电影分布可以看出，从1880年左右以来，电影的数量基本上是逐年增长的趋势，特别是进入21实际以来，增长速度很快（出现一段下降是因为2017年的完整数据收集不完整）。
下面再分析数据集中的电影的区域分布，利用一个比较强的可视化工具plotly，画出电影数量的区域分布，因为美国的电影产出相对其他国家高出太多，所以画图是先忽略了美国，这样画其他国家的数量之间的比较才会更加明显。

	data = [ dict(
	        type = 'choropleth',
	        locations = con_df['country'],
	        locationmode = 'country names',
	        z = con_df['num_movies'],
	        text = con_df['country'],
	        colorscale = [[0,'rgb(255, 255, 255)'],[1,'rgb(255, 0, 0)']],
	        autocolorscale = False,
	        reversescale = False,
	        marker = dict(
	            line = dict (
	                color = 'rgb(180,180,180)',
	                width = 0.5
	            ) ),
	        colorbar = dict(
	            autotick = False,
	            tickprefix = '',
	            title = '数量图例'),
	      ) ]
	
	layout = dict(
	    title = '电影数据集中电影数量分布（除美国外）',
	    geo = dict(
	        showframe = False,
	        showcoastlines = False,
	        projection = dict(
	            type = 'Mercator'
	        )
	    )
	)
	
	fig = dict( data=data, layout=layout )
	py.iplot( fig, validate=False, filename='d3-world-map' )
	plt.figure(figsize=(12,5))
	sns.barplot(x='country', y='num_movies', data=country)
	plt.show()
	####除去美国外，英国。法国、德国、意大利，亚洲的日本和印度，北美的巴西

![](https://i.imgur.com/oroLVyE.png)

![](https://i.imgur.com/ZuPNuyb.png)


## 推荐系统构建
在这次推荐系统的构建中，我们采用了三种算法来构建我们的推荐系统，基于这三种算法，包括基于用户投票的推荐算法、基于内容的推荐算法和协同过滤推荐算法，根据这些算法，最终来构建我们的电影推荐系统。

### 基于用户投票的推荐算法
作为国际知名的权威点评网站，在他们大名鼎鼎的TOP250榜单中，采用的就是贝叶斯算法，其公式如下：

![](https://i.imgur.com/YasOosa.png)

其中，WR为加权得分，R为该电影的用户投票平均得分，V为该电影的投票人数，m为最低评分个数，C为所有电影的平均得分。
这个算法的提出基于这样一个现实问题：热门电影与冷门电影的平均得分，是否真的可比？举例来说，一部好莱坞大片有10000个观众投票，一部小成本的文艺片只有100个观众投票。这两者的投票结果，怎么比较？如果使用"威尔逊区间"，后者的得分将被大幅拉低，这样处理是否公平，能不能反映它们真正的质量？一个合理的思路是，如果要比较两部电影的好坏，至少应该请同样多的观众观看和评分。既然文艺片的观众人数偏少，那么应该设法为它增加一些观众。

根据这个思路，这个算法相当于给每部电影增加了m个选票，并且每个评分为平均得分C，然后用现有观众的投票进行修正，即v*R/(v+m)部分，使得得分更加接近于真实情况。这种算法由于给每部电影增加了m个选票，拉近了不同电影之间投票人数的差异，使得投票人数较少的电影也有可能名列前茅。

这个算法借鉴了“贝叶斯推断”的思想，既然不知道投票结果，那就预先估计一个值，然后不断用新的信息修正，使它接近于正确值。在式子中，m可以看作是先验概率，每一次新的投票都是一个调整因子，使总体平均分不断向该项目的真实投票结果靠近。投票人数越多，该项目的"贝叶斯平均"就越接近算术平均，对排名的影响就越小。因此这种方法可以让投票较少的项目，能够得到相对公平的排名。

我们针对所有电影，类似于IMDB我们计算出了TOP250，下图为基于贝叶斯统计的用户投票排名算法得出的所有电影的TOP250中选取出的TOP10，其中电影名红色的为实际IMDB中进入TOP10的电影，可以看出有3部电影存在于IMDB的TOP10，绿色标注的电影为TOP11-15的电影，有3部。总的来说，贝叶斯统计得出的排名还是比较接近于IMDB的排名，但是由于我们的算法考虑的因素较少，所以还是有一定的区别。

![](https://i.imgur.com/Nd6D8YN.jpg)

进一步的，我们从电影数据集中，根据电影的genre属性值中，分离出电影所属的不同属性，所有电影的类型分布（TOP10）如下图所示

![](https://i.imgur.com/UMbWZN9.png)

可以看出，电影数据集中戏剧、喜剧、恐怖片、爱情片等数量较多，依次数量排名.针对数量超过3000的电影，我们也采取类似的方式计算了TOP250，部分电影类型的TOP10在下图中给出。

![](https://i.imgur.com/upDJLWF.png)

### 基于内容的推荐算法
基于投票排名的推荐算法给每个用户都是一样推荐按照TOP排名得出的电影，而不会根据特定的观众喜欢的电影去推荐相似的电影。为了能够给用户推荐相似的电影，我们首先需要对电影之间的相似性进行衡量，主要应用到电影的描述数据来完成基于内容的推荐，主要的实现过程包括：
•	对电影的关键词、描述信息、标语、主角、导演信息的提取
•	对上述信息进行词干提取
•	对上述信息进行特征抽取，转换成词向量
•	考虑评分情况，结合相似度完成推荐
首先我们对于电影的相关描述信息进行一个大致分析，制作了词云对所有电影的情况概览。
	
	df['title'] = df['title'].astype('str')
	df['overview'] = df['overview'].astype('str')
	title_corpus = ' '.join(df['title'])
	overview_corpus = ' '.join(df['overview'])
	from wordcloud import WordCloud, STOPWORDS
	title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(title_corpus)
	plt.figure(figsize=(16,8))
	plt.imshow(title_wordcloud)
	plt.axis('off')
	plt.show()

![](https://i.imgur.com/Fckk4jq.png)

	overview_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(overview_corpus)
	plt.figure(figsize=(16,8))
	plt.imshow(overview_wordcloud)
	plt.axis('off')
	plt.show()

![](https://i.imgur.com/5mNPVdG.png)

上面两幅图分别是电影的标题和电影简述画出的词云，可以看到电影标题中Love、Girl、Man、Life，Love作为最高频的词，毕竟大多数电影都有爱情这条线。在电影的简述中，find、life、one是最高频的词，可以给我们反映大多数电影的主题。

在获得电影的关键词、描述信息、标语、主角、导演信息之后，我们需要对这些信息进行词干提取。在语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，即得到单词最一般的写法。计算机科学领域有很多词干提取的相应算法，我们使用了一个面向英语的词干提取器stemming，使用Python的NLTK库的stemming算法，实现的效果为要识别字符串“cats”、“catlike”和“catty”提取出词根“cat”；“stemmer”、“stemming”和“stemmed”提取出词根“stem”。

	from nltk.stem.snowball import SnowballStemmer
	stemmer = SnowballStemmer('english')  #英语的词干提取

下一步需要将提取词干后的文档进行向量化处理，采用的是sklearn中的Countvectorizer。根据语料库中的词频排序从高到低进行选择，词汇表的最大含量由vocabsize超参数来指定，超参数minDF，则指定词汇表中的词语至少要在多少个不同文档中出现次数，产生文档关于词语的稀疏表示，在fitting过程中，countvectorizer将根据语料库中的词频排序选出前vocabsize个词，输出词向量。

	count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')
	count_matrix = count.fit_transform(smd['soup'])   #基于词向量统计的矩阵

当然，基于内容的推荐算法还需要考虑到电影的评分，不然仅仅根据电影之间的相似度，很有可能就会出现给观众推荐很相似的电影，但却是“烂片”的这种情况，基于这种考虑，以电影《The Godfather》（教父）以及《The Lord of the Rings: The Return of the King》（指环王：王者归来）为例，推荐的相应10部电影结果如下图所示。

![](https://i.imgur.com/JemBXZC.png)

可以看到，都推荐了同类型的电影，比如针对教父推荐了一些剧情、犯罪电影，而针对指环王推荐了一些动作、奇幻类的电影，而且这两部电影都有其他续集，比如针对《教父1》推荐了其续集《教父2》及《教父3》也都相应地推荐了，推荐的电影也都是高分电影.

### 协同过滤推荐
从应用的场景来看，基于内容的推荐算法更多地适用于用户根据关键字或者电影名字来搜索相应的电影，然后推荐系统来进行相应的推荐。基于需求个性角度来看，基于内容的推荐算法还不够个人化，用户需要的是更加符合个人偏好的推荐结果，可以根据用户之前的打分情况，更有针对性地推荐一些可能喜欢的电影，这种情况下，应用的最多的就是协同过滤算法。

协同过滤通过用户和产品及用户的偏好信息产生推荐策略，最基本的策略有两种：一种是找到具有类似品味的人所喜欢的物品；另一种是从一个人喜欢的物品中找出类似的物品，即基于用户的推荐技术（User CF）和基于物品的推荐技术（Item CF）。在我们这个应用场景中，有大量的电影信息，但是用户已经打分的电影只占总量很少的一部分，将用户打分和电影信息构成一个矩阵，那么这个矩阵会存在严重的稀疏性，经过计算大约在1.5%左右，基于这种考虑，我们采取Item-based协同过滤算法。同样由于矩阵的稀疏性，在数据量很大的情况下一般采用矩阵分解来减少运算量，采用PMF矩阵分解算法来完成这个目标。

采用的是surprise库中的SVD算法，但是我看了surprise库中的SVD算法介绍，其实更准确地说是PMF（Probabilistic Matrix Factorization）算法，即概率矩阵分解算法，所以这里对PMF进行相应的介绍。

假定每个用户u都有一个D维的向量，表示他对不同风格的电影的偏好，每个电影i也有一个D维的向量表示不同风格的用户对它的偏好。 于是电影的评分矩阵可以这样来估计：

![](https://i.imgur.com/Zj6DD9w.png)

p 和q就是D维的向量。用梯度下降法训练p和q，迭代几十次就收敛了。但是这样的SVD很容易就过拟合，所以需要加入正则化项：

![](https://i.imgur.com/gc43bcW.png)

这样每次迭代的时候，更新公式为：

![](https://i.imgur.com/MRt6hwJ.png)

采用5折交叉验证

	import pandas as pd
	import numpy as np
	from surprise import Reader, Dataset, SVD, evaluate
	from collections import defaultdict
	import warnings; warnings.simplefilter('ignore')
	
	
	reader = Reader()
	ratings = pd.read_csv('ratings_small.csv')
	
	#从DataFrame导入数据
	data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
	data.split(n_folds=5)
	trainset = data.build_full_trainset()
	#SVD算法
	algo = SVD()
	evaluate(algo, data, measures=['RMSE', 'MAE'])
	
	#训练模型
	algo.train(trainset)
	#对用户未评价的电影生成测试集
	testset = trainset.build_anti_testset()
	predictions = algo.test(testset)  #预测测试集结果
	
	
	def get_top_n(predictions, n=10):
	    '''对预测结果中的每个用户，返回n部电影，默认n=10
	    返回值一个字典，包括：
	    keys 为原始的userId，以及对应的values为一个元组
	        [(raw item id, rating estimation), ...].
	    '''
	
	    # 预测结果取出，对应每个userId.
	    top_n = defaultdict(list)
	    for uid, iid, true_r, est, _ in predictions:
	        top_n[uid].append((iid, est))
	    # 排序取出前n个
	    for uid, user_ratings in top_n.items():
	        user_ratings.sort(key=lambda x: x[1], reverse=True)
	        top_n[uid] = user_ratings[:n]
	    return top_n
	
	top_n = get_top_n(predictions, n=10)
	rec_result=np.zeros((671,11))  #定义二维矩阵来存放结果
	i=0
	for uid, user_ratings in top_n.items():
	    rec_result[i,0]=uid
	    rec_result[i,1:]=[iid for (iid, _) in user_ratings]
	    i=i+1
	rec_result=rec_result.astype('int')
	
	#转变成DataFrame
	rec_result=pd.DataFrame(rec_result,columns=['userId','rec1','rec2','rec3','rec4','rec5',
	                                          'rec6','rec7','rec8','rec9','rec10'])

算法运行结果：

![](https://i.imgur.com/wgzL9uA.png)


## 简单的电影点评网站构建
整体框架

![](https://i.imgur.com/bFDYLcO.png)


MySQL是一个关系型数据库管理系统，也是一种WEB应用最好的数据库。数据库作为中间件，搭建在寝室的主机，便于小组成员之间使用，其操纵代码：

	import pymysql
	import pandas as pd
	#连接数据库
	conn = pymysql.connect(host='10.110.43.140',port= 3306,user = '###',passwd='####',db='sys') #db：库名，用户名和密码这里我打了马赛克了，嘻嘻
	#创建游标
	cur = conn.cursor()
	df=pd.read_sql('SELECT * FROM db_movies.tb_movies;',conn)
	cur.close()
	conn.close()

小伙伴应用Django框架，是一个开放源代码的Web应用框架，由Python写，时间仓促的情况下赶出了一个还是很不错的页面，点赞。

![](https://i.imgur.com/qF14GhU.png)

详细代码可见个人[github](https://github.com/lkj1114889770/File_Recommend)。






