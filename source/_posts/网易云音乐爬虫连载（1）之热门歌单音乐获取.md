---
title: 网易云音乐爬虫连载（1）之热门歌单音乐获取
date: 2017-10-25 16:30:47
tags:
	- 爬虫
	- 音乐
---
最近想做文本挖掘方面的工作，想到了获取网易云音乐平台的用户评论以及音乐数据，这可以作为一个文本挖掘以及推荐系统的很好的数据来源。诚然，获取大量的数据涉及到内容还是挺多的，因此从本文开始做一个连载，记录今后对网易云音乐数据的爬取，以及今后对于获取的数据进行分析，作为机器学习的素材进一步处理。

作为连载的第一篇，首先就是介绍基本的网易云音乐信息获取，以及音乐评论的获取。
<!-- more -->

为了获得大量的音乐数据，从网易云音乐首页的热门歌单中入手，获取音乐信息。

![](https://i.imgur.com/IFZFT1q.png)

用谷歌开发者工具发现，其实获取热门歌单的时候，网易云的请求包中的网址并不是截图中浏览器的http://music.163.com/#/discover/playlist，而是http://music.163.com/discover/playlist

![](https://i.imgur.com/XeyP1qq.png)

所以说爬虫的时候，不能单纯看浏览器的url，还是得看真实发送的请求包中的数据。

	import requests
	from bs4 import BeautifulSoup
	import codecs
	url = 'http://music.163.com/discover/playlist'
	url_top = 'http://music.163.com'
	headers = {
	    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
	    'Accept-Language': 'zh-CN,zh;q=0.9',
	    'Host': 'music.163.com',
	    "User-Agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3218.0 Safari/537.36',
	}


分析获取的html文档，采用的是Beautiful Soup，这个时候首先查看html文档，找到歌单数据所在位置。

![](https://i.imgur.com/vxTTQWY.png)

发现一个id属性，id属性在一个html文档中是独一无二的，可以据此定位找出我们要的歌单信息。

	a=requests.get(url,headers=headers)
	html = a.content
	soup=BeautifulSoup(html,'lxml')
	playlist={}
	f=codecs.open('playlist.txt','w',encoding='utf-8')
	PlaylistBlock = soup.select('#m-pl-container')[0].select('.msk')
	for piece in PlaylistBlock:
	    playlist[piece.get('title')]=piece.get('href')
	    f.write(piece.get('title')+': '+piece.get('href')+'\n')
	f.close()

现在仅仅是获取了第一页的热门歌单：

![](https://i.imgur.com/OK1vv6L.png)

歌单的链接都是相对链接，只要加上http://music.163.com就可以访问到相应的具体歌单，来进一步获取歌单内的歌曲。

![](https://i.imgur.com/RKF87RZ.png)

	f=codecs.open('musiclist.txt','w',encoding='utf-8')
	musiclist = []
	
	for description,url_music in playlist.items():
	    html = requests.get(url_top+url_music,headers=headers).content
	    soup = BeautifulSoup(html,'lxml')
	    songs= soup.find('ul',{'class':'f-hide'}).find_all('a')
	    music={}
	    f.writelines(description+'\n')
	    for song in songs:
	        music[song['href']] = song.string
	        a = music_info_get(url_top+song['href'],headers)
	        f.write(song['href']+':  '+song.string+a+'\n')
	    musiclist.append(music)
	    f.write('---------------------------------'+'\n')
	
	f.close()

由于在歌单页面没有查到歌曲的对应歌手、专辑信息（理论上应该有的，但是我并没有找到），所以考虑进一步进到歌曲页面，可以看到歌曲的详细信息，后期主要在歌曲页面进行信息获取，所以在这里先进到歌曲页面获取歌手、专辑信息。

以赵雷的《成都》为例，进入歌曲页面

![](https://i.imgur.com/0lHWxPm.png)


提取出歌曲的详细信息：

	def music_info_get(url,headers):
	    html = requests.get(url, headers=headers).content
	    soup = BeautifulSoup(html, 'lxml')
	    a = soup.find('meta', {'name': 'description'})['content']
	    return a

《成都》的网页中包含歌词、评论等信息，但是在请求返回的数据包中并没有见到，刷新《成都》页面其实有很多请求，再仔细查看之后，在其他请求中看到了包括评论以及歌词信息。

![](https://i.imgur.com/T59hAoY.png)

在这个post请求包中，返回数据中有《成都》的音乐评论，post和get方式不同，post需要带参数，在request的header中可以看到有两个参数，

![](https://i.imgur.com/mtOQ9xQ.png)

将这个数据带上，用request.post也确实得到了评论数据。

	params = '5iLo/oxg1fK3aTLbh99GhtE6AnWBnEGVKMt4iDi6Qm9ag54eFjI/XRn2rI6QOAk8Zj6u2eS7NkRu04mUakNwntZMQrf9f6cdN6PWZuB16f0CgA0N/5IOl7tUXKZCbsduXzfpYCExtIvLDlOeu9LkGpUksFW3O0zq5ZTjRc1MrB49sxRvF8NA+U9LIMvhJHmO'
	encSecKey = '3ae5b6afde65dede52224db59c2cc8e46aac937dd95915ba6538859aa0615cb6aa938a118fd6f473256fc5cf95d8c3821b07264d7189c07db922088b711a357e3f2092e5a10df5e3d6008a0314adcb8817fc3fe14a2ee657a0a2221597cc51a78534043a1429484a251e4b2b9128fe042d821b7e862114207773cbdba951c8a2'
	data={"params": params, "encSecKey": encSecKey}
	a = requests.post(url,headers=headers,data=data)

但是post所带上的参数data，看起来应该是跟数据加密相关，每个请求应该不一样，在爬取大量音乐的时候没办法对每首歌的参数都去手动获得，这里在网上[https://www.zhihu.com/question/36081767](https://www.zhihu.com/question/36081767)看到了一个方法，之后可以参照来实现。
得到的评论数据：
![](https://i.imgur.com/RIr58UA.png)
还有就是请求歌曲的评论数据的url，对《成都》来说是http://music.163.com/weapi/v1/resource/comments/R_SO_4_436514312?csrf_token=，其规律就是R_SO_4_之后的数字为歌曲对应的id，比如《成都》的URL：http://music.163.com/#/song?id=436514312

爬虫进行到这，仅仅是从歌单到歌曲，再到歌曲信息以及评论单纯地获取了一遍，对于今后大量歌单大规模的爬取，还需要考虑很多，比如多线程爬虫，结合数据库的爬虫数据存储，以及对于长时间爬虫如何应对发爬虫策略，仅仅是获取数据就还有这么多坑，先Mark一下，留着坑慢慢填。