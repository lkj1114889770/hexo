---
title: AdaBoost算法笔记
date: 2017-10-22 15:35:19
tags:
	- 机器学习
	- 集成学习
---
AdaBoost属于机器学习中的集成学习，其基本思想是基于“弱学习算法”集成强化为“强学习算法”，在分类中就是组合弱分类器得到强分类器。实际中很容易得到正确率不是非常高的弱分类器(当然，通常来说at least >50%），通过Adaboost算法，来组合得到正确率很高的强分类器。
<!-- more -->
从上述就可以知道，AdaBoost算法有两部分，顶层是主算法，底层是其他的分类算法，可以是决策树，SVM之类的算法

![](https://i.imgur.com/YrWP4SU.jpg)

AdaBoost算法的核心思想就是：加大分类错误数据集的权重，以便迭代中进一步分类；加大正确率高的分类器在最终分类结果表决中的权重。

对算法进一步阐述，考虑简单的二分类问题。

1. 初始化数据集的权值分布，初始化为相等的数值

![](https://i.imgur.com/ryYjqNg.png)

1. 使用训练集学习，得到基本分类器Gm,计算其分类误差率

![](https://i.imgur.com/kelrKCR.png)

3. 计算分类器Gm的系数

![](https://i.imgur.com/4cl86ii.png)

am随着em增大而减小，这样分类器在最终组合分类器中权重会下降，同时还需要据此来更新数据集的权重。

4. 更新数据集权重分布

![](https://i.imgur.com/amKwnm4.png)

其中Zm为规范化因子，是的Dm+1成一个概率分布。

5. 构造弱分类器的组合
通过权重值的线性组合得到强分类器
![](https://i.imgur.com/0cjwKb5.png)

上述过程迭代M次，或者是分类精度达到要求


