---
title: 也看战狼2：爬取豆瓣影评做词云
date: 2017-08-08 22:47:02
tags: 
	- 电影
	- 爬虫
	- python
---

![](http://i.imgur.com/x7xGFON.jpg)


《战狼2》上映的第二天就去看了，当时觉得还不错，不管是打斗场景还是故事情节，看的都很过瘾，个人觉得可以给4星半。但是这段时间一直看到晚上对《战狼2》各种各样的不同的评论，因此闲暇之余，用爬虫获取了截止于2017.8.8号的豆瓣用户的近14万的评论，对其中的关键词做成了词云。
<!-- more -->

## python爬虫爬取评论代码
    
	import requests
    from bs4 import BeautifulSoup
    import codecs
    import time

    absolute_url = 'https://movie.douban.com/subject/26363254/comments'
    url = 'https://movie.douban.com/subject/26363254/comments?start={}&limit=20&sort=new_score&status=P'
    header={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:54.0) Gecko/20100101 Firefox/54.0','Connection':'keep-alive'}




	def html_prase(html, struct):
	    soup=BeautifulSoup(html,'lxml')
	    comment_nodes = []
	    comment_nodes = soup.select(struct)
	    xiangdui_link_nodes= soup.select('#paginator > a')[0].get('href')
	    return comment_nodes,xiangdui_link_nodes

	if __name__ == '__main__':
		#读取cookie数据
	    f_cookies = open('cookie.txt', 'r')
	    cookies = {}
	    for line in f_cookies.read().split(';'):
	        name, value = line.strip().split('=', 1)
	        cookies[name] = value
	    f = codecs.open("comments.txt", 'a', encoding='utf-8')
	    html = requests.get(url, cookies=cookies, headers=header).content
	    comment_nodes=[]
	    xiangdui_links=[]
		#获取评论
	    comment_nodes,xiangdui_link_nodes = html_prase(html , '.comment > p')
	    soup = BeautifulSoup(html, 'lxml')
	    comment_list = []
	    for node in comment_nodes:
	        comment_list.append(node.get_text().strip().replace("\n", "") + u'\n')
	    while(xiangdui_link_nodes!=[]):#每次查看是否有后页，即不断往深处挖掘，获取数据
	        xiangdui_link = soup.select('#paginator > a')[0].get('href') #取出后页的相对链接
	        xiangdui_links.append(xiangdui_link)
	        time.sleep(1)
	        html = requests.get(absolute_url+xiangdui_link_nodes, cookies=cookies, headers=header).content
	        soup = BeautifulSoup(html, 'lxml')
	        comment_nodes, xiangdui_link_nodes = html_prase(html, '.comment > p')
	        for node in comment_nodes:
	            comment = node.get_text().strip().replace("\n", "") + u'\n'
	            comment_list.append(comment)
	            f.writelines(comment)	
    

在抓取豆瓣影评的时候，一开始我是直接对URL爬虫，仅仅是加了一个header，抓取一段时间，豆瓣的反爬虫策略就将我的IP封掉了，所以我又加入了cookie字段。cookie是一个字典类型的数据，可以以比较简单的方式获取。打开要浏览的豆瓣页面，点击登陆页面后，打开Chrome的开发者模式，开始监听登陆时候的http请求和响应。
![](http://i.imgur.com/AK1Gm88.png)

这个时候，在Cookie字段可以找到cookie数据，复制后存为txt文件，然后写代码读取txt文件，并存为字典格式数据。

    f_cookies = open('cookie.txt', 'r')
    cookies = {}
    for line in f_cookies.read().split(';'):
        name, value = line.strip().split('=', 1)
        cookies[name] = value

用requests对网页进行爬虫抓取之后，此后就是利用Beautifulsoup对获取的html进行解析，获取豆瓣用户评论，以及后页的链接。
![](http://i.imgur.com/8gnYM4R.png)

对《战狼2》的豆瓣影评链接进行分析，发现每一页链接都是如上图的组成，网页解析可以获取后面的红色字段，实现不断向后页爬虫。最后爬虫结果得到18M左右的数据。

## jieba模块提取评论内容关键词
    # -*- coding: utf-8 -*-
	"""
	Created on Wed Aug  9 09:51:51 2017
	
	@author: lkj
	"""
	import codecs
	import jieba
	import matplotlib.pyplot as plt  
	import matplotlib as mpl 
	import numpy as np 
	from collections import Counter
	
	zhfont1 = mpl.font_manager.FontProperties(fname='C:\Windows\Fonts\simsun.ttc')
	
	def draw_bar(labels,quants):  
	    width = 0.4  
	    ind = np.linspace(0.5,9.5,10)  
	    # make a square figure  
	    fig = plt.figure(1)  
	    ax  = fig.add_subplot(111)  
	    # Bar Plot  
	    ax.bar(ind-width/2,quants,width,color='green')  
	    # Set the ticks on x-axis  
	    ax.set_xticks(ind)  
	    ax.set_xticklabels(labels,fontproperties=zhfont1)  
	    # labels  
	    ax.set_xlabel(u'关键词',fontproperties=zhfont1)  
	    ax.set_ylabel(u'评论数量',fontproperties=zhfont1)  
	    # title  
	    ax.set_title(u'筛选后的TOP10关键词', bbox={'facecolor':'0.8', 'pad':5},fontproperties=zhfont1)  
	    #plt.legend(prop=zhfont1)
	    plt.grid(True)  
	    plt.show()   
	
	word_lists = []  # 关键词列表
	with codecs.open('comments.txt', 'r', encoding='utf-8') as f:
	    Lists = f.readlines()  # 文本列表
	    for List in Lists:
	        cut_list = list(jieba.cut(List))
	        for word in cut_list:
	            word_lists.append(word)
	word_lists_set = set(word_lists)  # 去除重复元素
	sort_count = []
	word_lists_set = list(word_lists_set)
	length = len(word_lists_set)
	print(u"共有%d个关键词" %length)
	k = 1
	for w in word_lists_set:
	    sort_count.append(w + u':' + str(word_lists.count(w)) + u"次\n")
	    print (u"%d---" % k + w + u":" + str(word_lists.count(w)) + u"次")
	    k += 1
	with codecs.open('count_word.txt', 'w', encoding='utf-8') as f:
	    f.writelines(sort_count)
	#先取出前100关键词，再进行人为筛选
	key_words_TOP100=[]
	key_words_TOP100=Counter(word_lists).most_common(100)
	key_words_shaixuan=[key_words_TOP100[6],key_words_TOP100[24],key_words_TOP100[25],
	                    key_words_TOP100[30],key_words_TOP100[39],key_words_TOP100[52],
	                    key_words_TOP100[60],key_words_TOP100[77],key_words_TOP100[78],
	                    key_words_TOP100[94]]
	labels = []
	quants = []
	for i in range(10):
	    labels.append(key_words_shaixuan[i][0])
	    quants.append(key_words_shaixuan[i][1])
	draw_bar(labels,quants)

绘制柱形图的时候需要指定字体，不然会出现中文乱码。对关键词TOP100需要进行人为筛选，因为jieba分词会出现很多诸如“我们”之类的在这里无意义的词汇，人为筛选出TOP10关键词如下：

![](http://i.imgur.com/M2v23dA.png)

从关键词来看，大多数网友还是看好这部电影的，认为这是大场面的动作戏，达到了好莱坞大片水平，当然也不乏网友认为这是满足吴京个人英雄主义的意淫。

## 绘制词云
    # -*- coding: utf-8 -*-
	"""
	Created on Tue Aug  8 21:46:04 2017
	
	@author: lkj
	"""
	
	# -*- coding:utf-8 -*-
	import codecs
	
	import jieba
	from scipy.misc import imread
	from wordcloud import WordCloud
	
	
	# 绘制词云
	def save_jieba_result():
	    # 设置多线程切割
	    #jieba.enable_parallel(4)
	    with codecs.open('comments.txt', encoding='utf-8') as f:
	        comment_text = f.read()
	    cut_text = " ".join(jieba.cut(comment_text))  # 将jieba分词得到的关键词用空格连接成为字符串
	    with codecs.open('jieba.txt', 'a', encoding='utf-8') as f:
	        f.write(cut_text)
	
	
	def draw_wordcloud2():
	    with codecs.open('jieba.txt', encoding='utf-8') as f:
	        comment_text = f.read()
	
	    color_mask = imread("zhanlang2.jpg")  # 读取背景图片
	
	    stopwords = [u'就是', u'电影', u'你们', u'这么', u'不过', u'但是', u'什么', u'没有', u'这个', u'那个', u'大家', u'比较', u'看到', u'真是',
	                 u'除了', u'时候', u'已经', u'可以',u'湄公河']
	    cloud = WordCloud(font_path="MSYH.TTF", background_color='white',
	                      max_words=2000, max_font_size=200, min_font_size=4, mask=color_mask,stopwords=stopwords)
	    word_cloud = cloud.generate(comment_text)  # 产生词云
	    word_cloud.to_file("zhanlang2_cloud.jpg")
	
	save_jieba_result()
	draw_wordcloud2()

词云的绘制需要需要指定font_path，不然会出现中文乱码，我在网上下好微软雅黑的字体（.TTF文件）一并放在目录下调用。


## 也想说两句
电影的后面，红旗飘扬，进过敌战区的吴京身披五星红旗大摇大摆经过，那一刻真的为作为一个中国人而感到自豪。
![](http://i.imgur.com/bUYDPL5.jpg)
有人觉得看的剧情尴尬，完全是吴京个人英雄主义的表现，但是在《看速度与激情》一个人干翻整个俄罗斯核基地为什么不觉得尴尬呢？我们接受了太多的美国大片以及美国的个人英雄主义的意识形态的输出，有美国队长能够拯救世界，为什么中国队长不行呢？现在我们的国家也是越来越强大，很欣慰能有《湄公河行动》、《战狼2》这样的主旋律大片，虽然有很多不好的声音，也有越来越多的人被感染，而认同，《战狼2》上映两周就登顶国内票房冠军就印证了这一切。

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=520 height=86 src="//music.163.com/outchain/player?type=2&id=491295324&auto=0&height=66"></iframe>

不可否认，在调动自然流露爱国情愫上，《战狼2》是成功的，它在我不知不觉的情感代入里推揉了我的泪腺。当舰长青筋怒暴将憋在心中已久的爱国情感汇成一句“开火”时，男主冷锋独自潜入暴乱的非洲国家拼尽全力解救侨胞、身处险境孤立无援的那种英雄悲凉绝望感，瞬间倾倒而出。祖国，在这一刻，有了最真切感受。

当然，《战狼2》还是存在很多瑕疵，但是也为国产电影树立了一个新的标杆，相信国产大片也会越来越好，期待有更多像《战狼2》这样优秀的电影。

最后，很喜欢这部电影的结尾，中华人民共和国的护照。
![](http://i.imgur.com/DG2aone.jpg)